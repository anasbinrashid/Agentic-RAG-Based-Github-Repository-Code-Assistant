[
  {
    "chunk_id": "d9a6ca0fc524",
    "filename": "Code.c",
    "file_path": "Code.c",
    "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <immintrin.h>\n\n#define N 1024\n\nvoid scalar_2Dimplementation(float **A, float **B, float **C) \n{\n    float *A_T = (float *)malloc(N * N * sizeof(float));\n\n    for (int i = 0; i < N; i++) \n    {\n        for (int j = 0; j < N; j++) \n        {\n            A_T[j * N + i] = A[i][j];\n        }\n    }\n\n    for (int i = 0; i < N; i++) \n    {\n        for (int j = 0; j < N; j++) \n        {\n            C[i][j] = A_T[i * N + j] * B[i][j];\n        }\n    }\n\n    free(A_T);\n}\n\nvoid scalar_1Dimplementation(float *A, float *B, float *C) \n{\n    float *A_T = (float *)malloc(N * N * sizeof(float));\n\n    for (int i = 0; i < N; i++) \n    {\n        for (int j = 0; j < N; j++) \n        {\n            A_T[j * N + i] = A[i * N + j];\n        }\n    }\n\n    for (int i = 0; i < N * N; i++) \n    {\n        C[i] = A_T[i] * B[i];\n    }\n\n    free(A_T);\n}\n\nvoid simd_implementation(float **A, float **B, float **C) \n{\n    float *A_T = (float *)malloc(N * N * sizeof(float));\n\n    for (int i = 0; i < N; i += 8) \n    { \n        for (int j = 0; j < N; j++) \n        {\n            __m256 row = _mm256_loadu_ps(&A[j][i]); \n            \n            for (int k = 0; k < 8; k++) \n            {\n                A_T[(i + k) * N + j] = ((float*)&row)[k];  \n            }\n        }\n    }\n\n    for (int i = 0; i < N; i++) \n    {\n        for (int j = 0; j < N; j += 8) \n        {  \n            __m256 a = _mm256_loadu_ps(&A_T[i * N + j]);\n            __m256 b = _mm256_loadu_ps(&B[i][j]);\n            __m256 c = _mm256_mul_ps(a, b);\n            _mm256_storeu_ps(&C[i][j], c);",
    "start_line": 1,
    "end_line": 75,
    "language": "c",
    "chunk_type": "chunk_1",
    "size_chars": 1599,
    "size_lines": 75,
    "created_at": "2025-07-10T14:34:28.178719",
    "repo_name": "SIMD-Optimization-Matrix-Transposition-and-Multiplication-C"
  },
  {
    "chunk_id": "b306a7e70445",
    "filename": "Code.c",
    "file_path": "Code.c",
    "content": "    }\n\n    for (int i = 0; i < N; i++) \n    {\n        for (int j = 0; j < N; j += 8) \n        {  \n            __m256 a = _mm256_loadu_ps(&A_T[i * N + j]);\n            __m256 b = _mm256_loadu_ps(&B[i][j]);\n            __m256 c = _mm256_mul_ps(a, b);\n            _mm256_storeu_ps(&C[i][j], c);\n        }\n    }\n\n    free(A_T);\n}\n\n\nint main() \n{\n    srand(time(NULL));\n\n    printf(\"\\n\\nN = %d\", N);\n\n    float **A = (float **)malloc(N * sizeof(float *));\n    float **B = (float **)malloc(N * sizeof(float *));\n    float **C = (float **)malloc(N * sizeof(float *));\n    \n    for (int i = 0; i < N; i++)\n    {\n        A[i] = (float *)malloc(N * sizeof(float));\n        B[i] = (float *)malloc(N * sizeof(float));\n        C[i] = (float *)malloc(N * sizeof(float));\n    }\n\n    for (int i = 0; i < N; i++) \n    {\n        for (int j = 0; j < N; j++) \n        {\n            A[i][j] = (float)rand() / RAND_MAX;\n            B[i][j] = (float)rand() / RAND_MAX;\n        }\n    }\n\n    clock_t start = clock();\n    scalar_2Dimplementation(A, B, C);\n    clock_t end = clock();\n    \n    printf(\"\\n\\nScalar 2D time: %f seconds\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n\n    float *A_flat = (float *)malloc(N * N * sizeof(float));\n    float *B_flat = (float *)malloc(N * N * sizeof(float));\n    float *C_flat = (float *)malloc(N * N * sizeof(float));\n\n    for (int i = 0; i < N; i++) \n    {\n        for (int j = 0; j < N; j++) \n        {\n            A_flat[i * N + j] = A[i][j];\n            B_flat[i * N + j] = B[i][j];\n        }\n    }\n\n    start = clock();\n    scalar_1Dimplementation(A_flat, B_flat, C_flat);\n    end = clock();\n    \n    printf(\"\\nScalar 1D time: %f seconds\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n\n    start = clock();\n    simd_implementation(A, B, C);\n    end = clock();\n    \n    printf(\"\\nSIMD time: %f seconds\\n\\n\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n\n    for (int i = 0; i < N; i++) ",
    "start_line": 66,
    "end_line": 140,
    "language": "c",
    "chunk_type": "chunk_2",
    "size_chars": 1899,
    "size_lines": 75,
    "created_at": "2025-07-10T14:34:28.178772",
    "repo_name": "SIMD-Optimization-Matrix-Transposition-and-Multiplication-C"
  },
  {
    "chunk_id": "d8a84d007653",
    "filename": "Code.c",
    "file_path": "Code.c",
    "content": "    \n    printf(\"\\nScalar 1D time: %f seconds\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n\n    start = clock();\n    simd_implementation(A, B, C);\n    end = clock();\n    \n    printf(\"\\nSIMD time: %f seconds\\n\\n\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n\n    for (int i = 0; i < N; i++) \n    {\n        free(A[i]);\n        free(B[i]);\n        free(C[i]);\n    }\n\n    free(A);\n    free(B);\n    free(C);\n    free(A_flat);\n    free(B_flat);\n    free(C_flat);\n\n    return 0;\n}\n\n",
    "start_line": 131,
    "end_line": 157,
    "language": "c",
    "chunk_type": "chunk_3",
    "size_chars": 474,
    "size_lines": 27,
    "created_at": "2025-07-10T14:34:28.178784",
    "repo_name": "SIMD-Optimization-Matrix-Transposition-and-Multiplication-C"
  },
  {
    "chunk_id": "552491f2522d",
    "filename": "README.md",
    "file_path": "README.md",
    "content": "# SIMD Optimization for Matrix Transposition and Element-wise Multiplication\n\nA comprehensive implementation and performance analysis of matrix operations using SIMD (Single Instruction, Multiple Data) optimizations with AVX intrinsics in C.\n\n## Overview\n\nThis project demonstrates the optimization of matrix transposition followed by element-wise multiplication using three different approaches:\n\n1. **Scalar 2D Implementation** - Traditional nested loop approach with 2D arrays\n2. **Scalar 1D Implementation** - Optimized scalar version using 1D arrays for better cache locality  \n3. **SIMD Implementation** - Vectorized operations using AVX intrinsics for parallel processing\n\n### Problem Statement\nGiven two square matrices A and B of size N×N:\n- Compute the transpose of matrix A → A^T\n- Perform element-wise multiplication: C = A^T × B\n- Compare performance across different implementation strategies\n\n## Performance Results\n\n| Matrix Size | Scalar 2D (s) | Scalar 1D (s) | SIMD (s) | Speedup |\n|-------------|---------------|---------------|----------|---------|\n| 256×256     | 0.000517      | 0.000622      | 0.000172 | **3.00x** |\n| 512×512     | 0.002585      | 0.002046      | 0.000987 | **2.61x** |\n| 1024×1024   | 0.012183      | 0.014570      | 0.003914 | **3.11x** |\n| 2048×2048   | 0.057914      | 0.069366      | 0.014464 | **4.00x** |\n\n**Key Findings:**\n- SIMD consistently achieves 3-4x performance improvement\n- Performance gains increase with larger matrix sizes\n- Dynamic memory allocation prevents stack overflow for large matrices\n\n## Technical Implementation\n\n### Memory Management\n- **Dynamic Allocation**: Uses `malloc()` to handle large matrices on the heap\n- **Prevents Stack Overflow**: Avoids segmentation faults with large matrix sizes\n- **Contiguous Memory**: Ensures better cache performance\n\n### SIMD Optimizations\n- **AVX Intrinsics**: Processes 8 floating-point values simultaneously using `__m256`\n- **Vectorized Operations**: \n  - `_mm256_loadu_ps()` for loading data\n  - `_mm256_mul_ps()` for parallel multiplication\n  - `_mm256_storeu_ps()` for storing results\n- **Loop Unrolling**: Reduces loop overhead and improves throughput\n\n### Cache Optimization\n- **1D Array Layout**: Better memory locality compared to 2D arrays\n- **Sequential Access Patterns**: Minimizes cache misses during operations\n\n## Repository Structure\n\n```\n├── Code.c          # Complete implementation with all three methods\n├── Report.pdf      # Detailed analysis and performance results\n└── README.md       # This file\n```\n\n## Prerequisites\n\n### Hardware Requirements\n- CPU with AVX support (Intel Sandy Bridge or newer, AMD Bulldozer or newer)\n- Sufficient RAM for large matrix operations\n\n### Software Requirements\n- GCC compiler with AVX support\n- Linux/Ubuntu environment (tested on Ubuntu VM)\n- Standard C libraries\n\n## Compilation and Usage\n\n### Compile the program:\n```bash\ngcc -o matrix_simd Code.c -mavx -O3",
    "start_line": 1,
    "end_line": 75,
    "language": "markdown",
    "chunk_type": "chunk_1",
    "size_chars": 2930,
    "size_lines": 75,
    "created_at": "2025-07-10T14:34:28.211707",
    "repo_name": "SIMD-Optimization-Matrix-Transposition-and-Multiplication-C"
  },
  {
    "chunk_id": "0afc54d3604f",
    "filename": "README.md",
    "file_path": "README.md",
    "content": "### Software Requirements\n- GCC compiler with AVX support\n- Linux/Ubuntu environment (tested on Ubuntu VM)\n- Standard C libraries\n\n## Compilation and Usage\n\n### Compile the program:\n```bash\ngcc -o matrix_simd Code.c -mavx -O3\n```\n\n### Run the program:\n```bash\n./matrix_simd\n```\n\n### Compilation Flags Explained:\n- `-mavx`: Enables AVX instruction set\n- `-O3`: Maximum optimization level\n- `-o matrix_simd`: Output executable name\n\n## Understanding the Code\n\n### Scalar 2D Implementation\n```c\nvoid scalar_2Dimplementation(float **A, float **B, float **C)\n```\n- Traditional approach using nested loops\n- 2D array indexing with `A[i][j]`\n- Baseline for performance comparison\n\n### Scalar 1D Implementation  \n```c\nvoid scalar_1Dimplementation(float *A, float *B, float *C)\n```\n- Uses 1D arrays with manual indexing `A[i*N + j]`\n- Better cache locality than 2D approach\n- Demonstrates importance of memory layout\n\n### SIMD Implementation\n```c\nvoid simd_implementation(float **A, float **B, float **C)\n```\n- Vectorized operations using AVX intrinsics\n- Processes 8 elements per instruction\n- Achieves significant performance improvements\n\n## Key Learning Points\n\n1. **Memory Layout Matters**: 1D arrays can outperform 2D arrays due to better cache locality\n2. **SIMD Parallel Processing**: Processing multiple elements simultaneously provides substantial speedups\n3. **Dynamic Memory Allocation**: Essential for handling large datasets without stack limitations\n4. **Performance Scaling**: SIMD benefits increase with larger problem sizes\n\n## Potential Improvements\n\n- **Memory Alignment**: Use aligned memory allocation for optimal AVX performance\n- **Error Handling**: Add checks for malloc failures and invalid inputs\n- **Advanced SIMD**: Implement more sophisticated transposition algorithms\n- **Benchmarking**: Add more comprehensive timing and profiling tools\n\n## Educational Value\n\nThis project demonstrates:\n- Practical application of SIMD programming concepts\n- Performance optimization techniques in C\n- Memory management best practices\n- Comparative analysis methodologies\n- Real-world parallel computing applications\n\n## Contributing\n\nFeel free to:\n- Submit issues for bugs or improvements",
    "start_line": 66,
    "end_line": 140,
    "language": "markdown",
    "chunk_type": "chunk_2",
    "size_chars": 2195,
    "size_lines": 75,
    "created_at": "2025-07-10T14:34:28.211738",
    "repo_name": "SIMD-Optimization-Matrix-Transposition-and-Multiplication-C"
  },
  {
    "chunk_id": "e93c62c60b83",
    "filename": "README.md",
    "file_path": "README.md",
    "content": "- Practical application of SIMD programming concepts\n- Performance optimization techniques in C\n- Memory management best practices\n- Comparative analysis methodologies\n- Real-world parallel computing applications\n\n## Contributing\n\nFeel free to:\n- Submit issues for bugs or improvements\n- Propose optimizations or alternative implementations\n- Add support for different matrix sizes or data types\n- Enhance documentation or add examples\n",
    "start_line": 131,
    "end_line": 144,
    "language": "markdown",
    "chunk_type": "chunk_3",
    "size_chars": 436,
    "size_lines": 14,
    "created_at": "2025-07-10T14:34:28.211751",
    "repo_name": "SIMD-Optimization-Matrix-Transposition-and-Multiplication-C"
  }
]